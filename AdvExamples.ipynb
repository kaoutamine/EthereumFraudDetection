{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c68e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torchmetrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc, classification_report, ConfusionMatrixDisplay\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9639232",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.3\n",
    "shuffle_dataset  = True\n",
    "batch_size = 64\n",
    "max_epochs = 100\n",
    "input_size = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439b02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transaction(only_fraud=False):\n",
    "    df = pd.read_csv(\"transaction_dataset.csv\")\n",
    "\n",
    "    #Rename columns for easier access\n",
    "    df.columns = df.columns.str.strip().str.replace(' ','_').str.lower()\n",
    "\n",
    "    #Remove weird stuff \n",
    "    df.drop(columns=['unnamed:_0'], inplace=True)\n",
    "\n",
    "    #Remove duplicate accounts\n",
    "    df.drop_duplicates(subset=['address'], inplace=True)\n",
    "\n",
    "    #Remove accounts \n",
    "    df.drop(columns=['address'], inplace=True)\n",
    "\n",
    "    #Remove index\n",
    "    df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "    #Remove token names \n",
    "    df.drop(columns=['erc20_most_sent_token_type','erc20_most_rec_token_type'], inplace = True)\n",
    "\n",
    "    #Remove var=0 columns\n",
    "    df.drop(df.var(numeric_only=True)[df.var(numeric_only=True) == 0].index, axis = 1, inplace = True)\n",
    "\n",
    "    #Remove small distribution columns\n",
    "    small_distr_col = []\n",
    "    for col in df.columns[3:] :\n",
    "        if df[col].nunique() < 10:\n",
    "            small_distr_col.append(col)\n",
    "    df.drop(columns=small_distr_col,inplace = True)\n",
    "    \n",
    "    # Remove negative values \n",
    "    df[df<0] = None \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    if(only_fraud):\n",
    "        df.drop(df[df['flag'] == 0].index, inplace = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def train_test_split_indices(length, validation_split, shuffle_dataset = True, random_seed = 42):\n",
    "    # Creating data indices for training and validation splits.\n",
    "    indices = np.arange(length)\n",
    "    validation_size = int(validation_split * length)\n",
    "    if shuffle_dataset:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[validation_size:], indices[:validation_size]\n",
    "    return train_indices, val_indices\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, indices, augment = False, \n",
    "                 cols_median = None, cols_means = None, cols_std = None):\n",
    "        super().__init__()\n",
    "        self.augment = augment\n",
    "        \n",
    "        df = df.iloc[indices].copy()\n",
    "        \n",
    "        if(self.augment):\n",
    "            oversample = SMOTE()\n",
    "            df,y = oversample.fit_resample(df.iloc[:,1:],df.values[:,0])\n",
    "        else: \n",
    "            y,df  = df.values[:, 0],df.iloc[:, 1:]\n",
    "        \n",
    "        if any(param is None for param in [cols_median, cols_means, cols_std]):\n",
    "            self.cols_median = df.median(numeric_only=True)\n",
    "            self.cols_means  = df.mean  (numeric_only=True)\n",
    "            self.cols_std    = df.std   (numeric_only=True)\n",
    "        else:\n",
    "            self.cols_median = cols_median\n",
    "            self.cols_means  = cols_means\n",
    "            self.cols_std    = cols_std\n",
    "            \n",
    "        df.fillna(self.cols_median, inplace = True)\n",
    "        df = (df - self.cols_means) / self.cols_std\n",
    "        \n",
    "        self.y = y\n",
    "        self.X = df.values                                    \n",
    "        \n",
    "    def get_cols_stats(self):\n",
    "        return self.cols_median, self.cols_means, self.cols_std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        \n",
    "        if self.augment:\n",
    "            pass\n",
    "        \n",
    "        return x, y\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, df, batch_size, train_indices, val_indices,augment):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.train_indices = train_indices\n",
    "        self.val_indices = val_indices\n",
    "        self.augment = augment\n",
    "    def get_stats(self):\n",
    "        return self.train_set.get_cols_stats()\n",
    "    \n",
    "    def setup(self, stage = None):\n",
    "        self.train_set = MyDataset(self.df, self.train_indices, augment = self.augment)\n",
    "        \n",
    "        train_cols_median, train_cols_means, train_cols_std = self.train_set.get_cols_stats()\n",
    "        \n",
    "        self.val_set = MyDataset(self.df, self.val_indices,\n",
    "                                 augment = False,\n",
    "                                 cols_median = train_cols_median,\n",
    "                                 cols_means  = train_cols_means,\n",
    "                                 cols_std    = train_cols_std)\n",
    "          \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set,\n",
    "                          batch_size = self.batch_size,\n",
    "                          shuffle = True,\n",
    "                          num_workers = 8,\n",
    "                          pin_memory = True)\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set,\n",
    "                          batch_size = self.batch_size,\n",
    "                          shuffle = False,\n",
    "                          num_workers = 8,\n",
    "                          pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd8e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,only_fraud=False,augment=False,validation_split=validation_split,\n",
    "          shuffle_dataset=False,batch_size=batch_size,\n",
    "         max_epochs=max_epochs,input_size=input_size):\n",
    "\n",
    "    df = load_transaction(only_fraud=only_fraud)\n",
    "\n",
    "\n",
    "    train_indices, val_indices = train_test_split_indices(length = len(df),\n",
    "                                                      validation_split = validation_split)        \n",
    "\n",
    "    data = MyDataModule(df,\n",
    "                        train_indices = train_indices,\n",
    "                        val_indices   = val_indices,\n",
    "                        batch_size    = batch_size,\n",
    "                        augment       = augment)\n",
    "    early_stopping = EarlyStopping('val_loss',patience=7)\n",
    "\n",
    "    trainer = pl.Trainer(log_every_n_steps       = 10,\n",
    "                         accelerator             = 'cpu',\n",
    "                         check_val_every_n_epoch = 1,\n",
    "                         enable_checkpointing    = False,\n",
    "                         max_epochs              = max_epochs,\n",
    "                         precision               = 64,\n",
    "                         callbacks               = [],\n",
    "                         num_sanity_val_steps    = 0,\n",
    "                         fast_dev_run            = False)\n",
    "\n",
    "    trainer.fit(model, data)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d5a6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='auc', gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training and using the classifier \n",
    "df = load_transaction()\n",
    "\n",
    "train_indices, val_indices = train_test_split_indices(length = len(df),\n",
    "                                                  validation_split = validation_split)  \n",
    "data = MyDataModule(df,\n",
    "                        train_indices = train_indices,\n",
    "                        val_indices   = val_indices,\n",
    "                        batch_size    = 1,\n",
    "                        augment       = False)\n",
    "data.setup()\n",
    "\n",
    "median, mean, std = data.get_stats()\n",
    "\n",
    "test_data_loader = data.val_dataloader()\n",
    "y_te = test_data_loader.dataset[:][1]\n",
    "x_te = test_data_loader.dataset[:][0]\n",
    "\n",
    "train_data_loader = data.train_dataloader()\n",
    "y_tr = train_data_loader.dataset[:][1]\n",
    "x_tr = train_data_loader.dataset[:][0]\n",
    "\n",
    "xgb_c = XGBClassifier(random_state=42,objective='binary:logistic',use_label_encoder=False,eval_metric='auc')\n",
    "xgb_c.fit(x_tr, y_tr.astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75e373df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(pl.LightningModule):\n",
    "    def __init__(self,classifier,input_size=33):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x.view(x.size(0), -1))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        #Create some kind of camouflage to hack the classifier\n",
    "        camouflage = self(x)\n",
    "        generated = x + camouflage\n",
    "        r_loss = F.mse_loss(torch.zeros_like(camouflage), camouflage)\n",
    "        \n",
    "        #We want the predictions to all be zero (i.e, non fraud)\n",
    "        preds = torch.tensor(self.classifier.predict(generated.detach())).double()\n",
    "        preds.requires_grad_()\n",
    "        c_loss = preds.mean()\n",
    "        \n",
    "        \n",
    "        self.log('r_loss', r_loss, prog_bar = True, on_step = False, on_epoch = True)\n",
    "        self.log('c_loss', c_loss, prog_bar = True, on_step = False, on_epoch = True)\n",
    "        \n",
    "        return (c_loss + 100*r_loss)/100\n",
    "    \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "433bd858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorBIG(pl.LightningModule):\n",
    "    def __init__(self,classifier,input_size=33):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x.view(x.size(0), -1))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        #Create some kind of camouflage to hack the classifier\n",
    "        camouflage = self(x)\n",
    "        generated = x + camouflage\n",
    "        r_loss = F.mse_loss(torch.zeros_like(camouflage), camouflage)\n",
    "        \n",
    "        #We want the predictions to all be zero (i.e, non fraud)\n",
    "        preds = torch.tensor(self.classifier.predict(generated.detach())).double()\n",
    "        preds.requires_grad_()\n",
    "        c_loss = preds.mean()\n",
    "        \n",
    "        \n",
    "        self.log('r_loss', r_loss, prog_bar = True, on_step = False, on_epoch = True)\n",
    "        self.log('c_loss', c_loss, prog_bar = True, on_step = False, on_epoch = True)\n",
    "        \n",
    "        return (c_loss + 100*r_loss)/100\n",
    "    \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c2f4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorHUGE(pl.LightningModule):\n",
    "    def __init__(self,classifier,input_size=33):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x.view(x.size(0), -1))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        #Create some kind of camouflage to hack the classifier\n",
    "        camouflage = self(x)\n",
    "        generated = x + camouflage\n",
    "        r_loss = F.mse_loss(torch.zeros_like(camouflage), camouflage)\n",
    "        \n",
    "        #We want the predictions to all be zero (i.e, non fraud)\n",
    "        preds = torch.tensor(self.classifier.predict(generated.detach())).double()\n",
    "        preds.requires_grad_()\n",
    "        c_loss = preds.mean()\n",
    "        \n",
    "        \n",
    "        self.log('r_loss', r_loss, prog_bar = True, on_step = False, on_epoch = True)\n",
    "        self.log('c_loss', c_loss, prog_bar = True, on_step = False, on_epoch = True)\n",
    "        \n",
    "        return (c_loss + 100*r_loss)/100\n",
    "    \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723f6ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:105: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 80.7 K\n",
      "--------------------------------------\n",
      "80.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "80.7 K    Total params\n",
      "0.645     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b083bf5b314d1ba34b4d8694831f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "gen = train(Generator(xgb_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64356b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 343 K \n",
      "--------------------------------------\n",
      "343 K     Trainable params\n",
      "0         Non-trainable params\n",
      "343 K     Total params\n",
      "2.749     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d732b58661948fdb69f5aaab9727da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genBIG = train(GeneratorBIG(xgb_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f021ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name   | Type       | Params\n",
      "--------------------------------------\n",
      "0 | layers | Sequential | 2.4 M \n",
      "--------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "19.546    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa6ba4b2a664adf85afa393dd864036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "genHUGE = train(GeneratorHUGE(xgb_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "287f9c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>    \n",
      "self._shutdown_workers()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "        Exception ignored in: if w.is_alive():self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>\n",
      "\n",
      "\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "Traceback (most recent call last):\n",
      "          File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
      "\n",
      "      File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "Exception ignored in: self._shutdown_workers()    <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      ": \n",
      "\n",
      "can only test a child process  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "AssertionErrorTraceback (most recent call last):\n",
      ": can only test a child process  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "\n",
      "\n",
      "        if w.is_alive():self._shutdown_workers()\n",
      "\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "Exception ignored in:     <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "    Exception ignored in: Traceback (most recent call last):\n",
      "\n",
      "if w.is_alive():  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>AssertionError\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>    \n",
      "\n",
      ":   File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "self._shutdown_workers()Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "\n",
      "    can only test a child process  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'    \n",
      "    self._shutdown_workers()    if w.is_alive():\n",
      "AssertionError\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "self._shutdown_workers():         if w.is_alive():\n",
      "can only test a child processassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "        AssertionErrorException ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>if w.is_alive():\n",
      "\n",
      "AssertionErrorTraceback (most recent call last):\n",
      ": : \n",
      "Exception ignored in: can only test a child process<function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "\n",
      "can only test a child process\n",
      "    Traceback (most recent call last):\n",
      "self._shutdown_workers()  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    \n",
      "      File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "Exception ignored in: self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>    \n",
      "\n",
      "if w.is_alive():Traceback (most recent call last):\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "\n",
      "        \n",
      "self._shutdown_workers()  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "if w.is_alive():\n",
      "      File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "\n",
      "AssertionError\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      ":     \n",
      "if w.is_alive():    can only test a child process\n",
      "AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      ":   File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "\n",
      "can only test a child process    AssertionErrorassert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      ": AssertionErrorcan only test a child process: \n",
      "can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fe084e093b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/Users/santangelx/opt/anaconda3/envs/ml2/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 changed XGBOOST output 228 times on 358 detected fraud accounts \n",
      "Model 2 changed XGBOOST output 226 times on 358 detected fraud accounts \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vr/3ycx2vtx4493y3jhjwnmc7gr0000gn/T/ipykernel_23818/991381791.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mcamo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0madv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mgood_camo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcamo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0mvalidate_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mbase_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m             \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1291\u001b[0m         )\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m             \u001b[0menable_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m         )\n\u001b[1;32m    903\u001b[0m         return self.get_booster().predict(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0mfeature_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0menable_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m         )\n\u001b[1;32m    624\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         )\n\u001b[0;32m--> 731\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_modin_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m         return _from_pandas_df(data, enable_categorical, missing, threads,\n\u001b[1;32m    733\u001b[0m                                feature_names, feature_types)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_is_modin_df\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_modin_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mmodin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_join\u001b[0;34m(*path_parts)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml2/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = [gen,genBIG,genHUGE]\n",
    "good_camo = []\n",
    "i=0\n",
    "for model in models:\n",
    "    i+=1\n",
    "    total = 0\n",
    "    detected_fraud = 0 \n",
    "    beat_xgb = 0 \n",
    "    for a,b in test_data_loader:\n",
    "        if(b==1):\n",
    "            total+=1\n",
    "            camo = gen(a).detach()\n",
    "            adv = a + camo \n",
    "            if(xgb_c.predict(a)==1):\n",
    "                if(xgb_c.predict(adv)==0):\n",
    "                    if(i==3):good_camo.append(camo)\n",
    "                    beat_xgb+=1\n",
    "                detected_fraud+=1\n",
    "    print(\"Model \"+str(i)+\" changed XGBOOST output \"+str(beat_xgb)+\" times on \"+str(detected_fraud)+\" detected fraud accounts \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d24eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "camo_mean = 0\n",
    "for camo in good_camo:\n",
    "    camo_mean+=camo.numpy()/len(good_camo)\n",
    "camo_mean = camo_mean[0]\n",
    "print(camo_mean)\n",
    "camo_mean=(camo_mean+mean)*std\n",
    "print(camo_mean - mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 16.9 K\n",
      "1 | discriminator | Discriminator | 14.7 K\n",
      "------------------------------------------------\n",
      "31.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "31.7 K    Total params\n",
      "0.253     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16ef37ddb93488bbf6d476f2fd4924b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, out_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x.view(x.size(0), -1))\n",
    "        return x\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x.view(x.size(0), -1)).flatten()\n",
    "\n",
    "        return x\n",
    "class GAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr: float  = 0.0001,\n",
    "        latent_dim = 50,\n",
    "        input_size = 33\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr         = lr\n",
    "\n",
    "        self.generator = Generator(latent_dim,input_size)\n",
    "        self.discriminator = Discriminator(input_size)\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        addrs, _ = batch\n",
    "\n",
    "        # sample noise\n",
    "        z = torch.randn(addrs.shape[0],self.latent_dim)\n",
    "        z = z.type_as(addrs)\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # ground truth result (ie: all fake)\n",
    "            # put on GPU because we created this tensor inside training_loop\n",
    "            valid = torch.ones(addrs.size(0), 1)\n",
    "            valid = valid.type_as(addrs)\n",
    "\n",
    "            # adversarial loss is binary cross-entropy\n",
    "            g_loss = self.adversarial_loss(self.discriminator(self(z)).view(addrs.size(0), 1), valid)\n",
    "            self.log(\"g_loss\", g_loss, prog_bar=True, on_step = False, on_epoch = True)\n",
    "            return g_loss\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "\n",
    "            # how well can it label as real?\n",
    "            valid = torch.ones(addrs.size(0), 1)\n",
    "            valid = valid.type_as(addrs)\n",
    "\n",
    "            real_loss = self.adversarial_loss(self.discriminator(addrs).view(addrs.size(0), 1), valid)\n",
    "\n",
    "            # how well can it label as fake?\n",
    "            fake = torch.zeros(addrs.size(0), 1)\n",
    "            fake = fake.type_as(addrs)\n",
    "\n",
    "            fake_loss = self.adversarial_loss(self.discriminator(self(z).detach()).view(addrs.size(0), 1), fake)\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            self.log(\"d_loss\", d_loss, prog_bar=True, on_step = False, on_epoch = True)\n",
    "            return d_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=self.lr)\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=self.lr)\n",
    "        \n",
    "        return [opt_g, opt_d], []\n",
    "    \n",
    "gan = train(GAN())\n",
    "gan.discriminator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51884c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorHUGEGAN(pl.LightningModule):\n",
    "    def __init__(self,classifier,discriminator,input_size=33):\n",
    "        super().__init__()\n",
    "        self.classifier = classifier\n",
    "        self.discriminator = discriminator\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, input_size),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x.view(x.size(0), -1))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        #Create some kind of camouflage to hack the classifier\n",
    "        camouflage = self(x)\n",
    "        generated = x + camouflage\n",
    "        r_loss = F.mse_loss(torch.zeros_like(camouflage), camouflage)\n",
    "        \n",
    "        #We want our generated account to ressemble other accounts \n",
    "        valid = torch.ones(x.size(0), 1)\n",
    "        valid = valid.type_as(x)\n",
    "\n",
    "        d_loss = F.binary_cross_entropy(self.discriminator(x).view(x.size(0), 1), valid)\n",
    "\n",
    "        #We want the predictions to all be zero (i.e, non fraud)\n",
    "        preds = torch.tensor(self.classifier.predict(generated.detach())).double()\n",
    "        preds.requires_grad_()\n",
    "        c_loss = preds.mean()\n",
    "        \n",
    "        self.log('d_loss', d_loss, prog_bar = True, on_step = False, on_epoch = True)\n",
    "        self.log('r_loss', r_loss, prog_bar = True, on_step = False, on_epoch = True)\n",
    "        self.log('c_loss', c_loss, prog_bar = True, on_step = False, on_epoch = True)\n",
    "        \n",
    "        return (c_loss + 10*r_loss + 10*d_loss)\n",
    "    \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "genHUGEGAN = train(GeneratorHUGEGAN(xgb_c,gan.discriminator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e5b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
